{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: UTF-8 -*-\n",
    "import time\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "from datetime import datetime,timedelta\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import random\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from operator import itemgetter\n",
    "import operator\n",
    "from sklearn.model_selection import KFold\n",
    "import scipy as sp\n",
    "import gc\n",
    "import lightgbm as lgb\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import csv\n",
    "#dl\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700288\n",
      "2700288\n"
     ]
    }
   ],
   "source": [
    "# #读取训练集\n",
    "# d28 = pd.read_csv('../280000.csv')\n",
    "# d29 = pd.read_csv('../290000.csv')\n",
    "\n",
    "# traindata = d28.copy()\n",
    "# print len(traindata)\n",
    "\n",
    "# #读取测试集\n",
    "# d30 = pd.read_csv('../300000.csv')\n",
    "# testdata = pd.concat([d29, d30])\n",
    "\n",
    "traindata = pd.read_csv('trainNN.csv')\n",
    "print len(traindata)\n",
    "\n",
    "#获取test label\n",
    "test_label_pd = pd.read_csv('trainLabel.csv')\n",
    "test_label = test_label_pd['label']\n",
    "print len(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700288\n"
     ]
    }
   ],
   "source": [
    "#获取app描述\n",
    "app_category = pd.read_csv('app200_category.csv')\n",
    "\n",
    "#获取训练app\n",
    "train_app_copy = traindata[['app_id']].copy()\n",
    "app_des = app_category[['app_id', 'des']].copy()\n",
    "train_app = pd.merge(train_app_copy, app_des, how = 'left', on = 'app_id')\n",
    "print len(train_app)\n",
    "#911479\n",
    "#895269\n",
    "#893540"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#获取train中出现的userid\n",
    "user_map = defaultdict(int)\n",
    "for i in traindata['user_id']:\n",
    "    user_map[int(i)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#获取train app matrix\n",
    "def get_app_matrix(train_app):\n",
    "    app_matrix = []\n",
    "    app_set = []\n",
    "    app_pos = defaultdict(int)\n",
    "\n",
    "    for i in train_app['des']:\n",
    "        str1 = i.split(',')\n",
    "        app_set += str1\n",
    "    app_set = list(set(app_set))\n",
    "    app_set = app_set[0:1000]\n",
    "    print len(app_set)\n",
    "\n",
    "    #记录每一个元素的位置\n",
    "    s_vec = 0\n",
    "    for i in app_set:\n",
    "        app_pos[i] = s_vec\n",
    "        s_vec += 1\n",
    "\n",
    "    len_vec = len(app_set)\n",
    "    print len_vec\n",
    "\n",
    "    for i in train_app['des']:\n",
    "        templist = []\n",
    "        for ln in range(0, len_vec):\n",
    "            templist.append(0)\n",
    "        str1 = i.split(',')\n",
    "        for j in str1:\n",
    "            templist[app_pos[j]] = 1\n",
    "        app_matrix.append(templist)\n",
    "\n",
    "\n",
    "    app_matrix = np.array(app_matrix)\n",
    "    return app_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "1000\n",
      "(2700288, 1000)\n"
     ]
    }
   ],
   "source": [
    "app_matrix = get_app_matrix(train_app)\n",
    "print app_matrix.shape\n",
    "np.save(\"app_matrix.npy\", app_matrix) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "1000000\n",
      "1100000\n",
      "1200000\n",
      "1300000\n",
      "1400000\n",
      "1500000\n",
      "1600000\n",
      "1700000\n",
      "1800000\n",
      "1900000\n",
      "2000000\n",
      "2100000\n",
      "2200000\n",
      "2300000\n",
      "2400000\n",
      "2500000\n",
      "2600000\n",
      "2700000\n",
      "(2700288, 1000)\n"
     ]
    }
   ],
   "source": [
    "#取出query\n",
    "query_1000 = defaultdict(int)\n",
    "query_temp_list = []\n",
    "query_map = {}\n",
    "with open('dataset_query/query.csv', 'rb') as csvfile:\n",
    "    sdata = csv.DictReader(csvfile)\n",
    "    for rec in sdata:\n",
    "        if (user_map[int(rec['user_id'])] == 0):\n",
    "            continue\n",
    "        temp_query = rec['query'].split(',')\n",
    "        if (not query_map.has_key(rec['user_id'])):\n",
    "            query_map[rec['user_id']] = []\n",
    "\n",
    "        query_map[rec['user_id']].append(temp_query)\n",
    "        \n",
    "        for i in temp_query:\n",
    "            query_1000[i] += 1\n",
    "            \n",
    "test_query_num = []\n",
    "for i in query_1000.keys():\n",
    "    test_query_num.append(query_1000[i])\n",
    "test_query_num.sort(reverse=True)\n",
    "\n",
    "min_query_1000 = test_query_num[999]\n",
    "\n",
    "#获取1000个词\n",
    "word_query = []\n",
    "for i in query_1000.keys():\n",
    "    if (query_1000[i] >= min_query_1000):\n",
    "        word_query.append(i)\n",
    "print len(word_query)\n",
    "\n",
    "len_query_1000 = len(word_query)\n",
    "\n",
    "#获取query word 位置\n",
    "query_pos = defaultdict(int)\n",
    "query_l = 0\n",
    "for i in word_query:\n",
    "    query_pos[i] = query_l\n",
    "    query_l += 1\n",
    "    \n",
    "#获取query矩阵 \n",
    "query_matrix = []\n",
    "num = 0\n",
    "for i in traindata['user_id']:\n",
    "    mx = []\n",
    "    num += 1\n",
    "    if (num %100000 == 0):\n",
    "        print num\n",
    "        \n",
    "    temp_mx = []\n",
    "    for tm in range(0, len_query_1000):\n",
    "        temp_mx.append(0)\n",
    "    if (not query_map.has_key(str(i))):\n",
    "        query_matrix.append(temp_mx)\n",
    "        continue\n",
    "        \n",
    "#     for j in query_map[str(i)]:\n",
    "#         temp_j = []\n",
    "#         for lj in range(0, len_query_1000):\n",
    "#             temp_j.append(0)\n",
    "#         for wj in j:\n",
    "#             temp_j[query_pos[wj]] = 1\n",
    "            \n",
    "#         temp_mx = list(map(lambda x: x[0] + x[1], zip(temp_mx, temp_j)))  \n",
    "    j = random.sample(query_map[str(i)], 1)[0]\n",
    "    temp_j = []\n",
    "    for lj in range(0, len_query_1000):\n",
    "        temp_j.append(0)\n",
    "    for wj in j:\n",
    "        temp_j[query_pos[wj]] = 1\n",
    "            \n",
    "    query_matrix.append(temp_j)\n",
    "    \n",
    "query_matrix = np.array(query_matrix)\n",
    "\n",
    "np.save(\"query_matrix.npy\", query_matrix) \n",
    "\n",
    "print query_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "1000000\n",
      "2000000\n",
      "(2700288, 1000)\n"
     ]
    }
   ],
   "source": [
    "#取出feed前1000个词\n",
    "feed_1000 = defaultdict(int)\n",
    "feed_temp_list = []\n",
    "feed_map = {}\n",
    "with open('dataset_newsfeed/newsfeed2.csv', 'rb') as csvfile:\n",
    "    sdata = csv.DictReader(csvfile)\n",
    "    for rec in sdata:\n",
    "        if (user_map[int(rec['user_id'])] == 0):\n",
    "            continue\n",
    "        temp_feed = rec['news'].split(',')\n",
    "        if (not feed_map.has_key(rec['user_id'])):\n",
    "            feed_map[rec['user_id']] = []\n",
    "\n",
    "        feed_map[rec['user_id']].append(temp_feed)\n",
    "        \n",
    "        for i in temp_feed:\n",
    "            feed_1000[i] += 1\n",
    "            \n",
    "test_feed_num = []\n",
    "for i in feed_1000.keys():\n",
    "    test_feed_num.append(feed_1000[i])\n",
    "test_feed_num.sort(reverse=True)\n",
    "\n",
    "min_feed_1000 = test_feed_num[999]\n",
    "\n",
    "#获取1000个词\n",
    "word_feed = []\n",
    "for i in feed_1000.keys():\n",
    "    if (feed_1000[i] >= min_feed_1000):\n",
    "        word_feed.append(i)\n",
    "print len(word_feed)\n",
    "\n",
    "len_feed_1000 = len(word_feed)\n",
    "\n",
    "#获取feed word 位置\n",
    "feed_pos = defaultdict(int)\n",
    "feed_l = 0\n",
    "for i in word_feed:\n",
    "    feed_pos[i] = feed_l\n",
    "    feed_l += 1\n",
    "    \n",
    "#获取feed矩阵 \n",
    "feed_matrix = []\n",
    "num = 0\n",
    "for i in traindata['user_id']:\n",
    "    mx = []\n",
    "    num += 1\n",
    "    if (num %1000000 == 0):\n",
    "        print num\n",
    "        \n",
    "    temp_mx = []\n",
    "    for tm in range(0, len_feed_1000):\n",
    "        temp_mx.append(0)\n",
    "    if (not feed_map.has_key(str(i))):\n",
    "        feed_matrix.append(temp_mx)\n",
    "        continue\n",
    "        \n",
    "    for j in feed_map[str(i)]:\n",
    "        temp_j = []\n",
    "        for lj in range(0, len_feed_1000):\n",
    "            temp_j.append(0)\n",
    "        for wj in j:\n",
    "            temp_j[feed_pos[wj]] = 1\n",
    "            \n",
    "        temp_mx = list(map(lambda x: x[0] + x[1], zip(temp_mx, temp_j)))  \n",
    "    \n",
    "    feed_matrix.append(temp_mx)\n",
    "    \n",
    "feed_matrix = np.array(feed_matrix)\n",
    "\n",
    "np.save(\"feed_matrix.npy\", feed_matrix) \n",
    "\n",
    "print feed_matrix.shape\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000000\n",
      "1000\n",
      "1000000\n",
      "2000000\n",
      "(2700288, 1000)\n"
     ]
    }
   ],
   "source": [
    "#取shopping前1000个词\n",
    "shop_1000 = defaultdict(int)\n",
    "shop_temp_list = []\n",
    "shop_map = {}\n",
    "with open('shopping_data2.csv', 'rb') as csvfile:\n",
    "    sdata = csv.DictReader(csvfile)\n",
    "    num = 0\n",
    "    for rec in sdata:\n",
    "        num += 1\n",
    "        if (num % 10000000 == 0):\n",
    "            print num\n",
    "        if (user_map[int(rec['user_id'])] == 0):\n",
    "            continue\n",
    "        temp_shopping = rec['shopping'].split(',')\n",
    "        \n",
    "        if (not shop_map.has_key(rec['user_id'])):\n",
    "            shop_map[rec['user_id']] = []\n",
    "\n",
    "        shop_map[rec['user_id']].append(temp_shopping)\n",
    "        for i in temp_shopping:\n",
    "            shop_1000[i] += 1\n",
    "            \n",
    "test_shop_num = []\n",
    "for i in shop_1000.keys():\n",
    "    test_shop_num.append(shop_1000[i])\n",
    "test_shop_num.sort(reverse=True)\n",
    "\n",
    "min_shop = test_shop_num[999]\n",
    "\n",
    "#获取1000个词\n",
    "word_shop = []\n",
    "for i in shop_1000.keys():\n",
    "    if (shop_1000[i] >= min_shop):\n",
    "        word_shop.append(i)\n",
    "print len(word_shop)\n",
    "\n",
    "len_shop = len(word_shop)\n",
    "\n",
    "#获取shop word 位置\n",
    "shop_pos = defaultdict(int)\n",
    "shop_l = 0\n",
    "for i in word_shop:\n",
    "    shop_pos[i] = shop_l\n",
    "    shop_l += 1\n",
    "    \n",
    "#获取shopping矩阵 \n",
    "shop_matrix = []\n",
    "num = 0\n",
    "for i in traindata['user_id']:\n",
    "    mx = []\n",
    "    num += 1\n",
    "    if (num %1000000 == 0):\n",
    "        print num\n",
    "        \n",
    "    temp_mx = []\n",
    "    for tm in range(0, len_shop):\n",
    "        temp_mx.append(0)\n",
    "    if (not shop_map.has_key(str(i))):\n",
    "        shop_matrix.append(temp_mx)\n",
    "        continue\n",
    "        \n",
    "    for j in shop_map[str(i)]:\n",
    "        temp_j = []\n",
    "        for lj in range(0, len_shop):\n",
    "            temp_j.append(0)\n",
    "        for wj in j:\n",
    "            temp_j[shop_pos[wj]] = 1\n",
    "            \n",
    "        temp_mx = list(map(lambda x: x[0] + x[1], zip(temp_mx, temp_j)))  \n",
    "    \n",
    "    shop_matrix.append(temp_mx)\n",
    "    \n",
    "shop_matrix = np.array(shop_matrix)\n",
    "print shop_matrix.shape\n",
    "\n",
    "np.save('shop_matrix.npy', shop_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2700288, 1000)\n",
      "(2700288, 1000)\n",
      "(2700288, 1000)\n",
      "(2700288, 1000)\n",
      "(2700288, 48)\n"
     ]
    }
   ],
   "source": [
    "#加载训练测试数据\n",
    "app_matrix = np.load('app_matrix.npy')\n",
    "feed_matrix = np.load(\"feed_matrix.npy\")\n",
    "shop_matrix = np.load('shop_matrix.npy')\n",
    "query_matrix = np.load('query_matrix.npy')\n",
    "xgb_matrix = np.load('xgb_feature_float.npy')\n",
    "\n",
    "print app_matrix.shape\n",
    "print feed_matrix.shape\n",
    "print shop_matrix.shape\n",
    "print query_matrix.shape\n",
    "print xgb_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "define model\n",
    "'''\n",
    "\n",
    "NUM_UNITS = 2000\n",
    "RES_DEPTH = 16\n",
    "batch_size = 128\n",
    "num_iterations = 10000001\n",
    "\n",
    "def resblock(x):\n",
    "    h = tf.contrib.layers.fully_connected(\n",
    "        inputs=tf.contrib.layers.batch_norm(tf.nn.relu(x)),\n",
    "        num_outputs=NUM_UNITS,\n",
    "#         normalizer_fn=tf.contrib.layers.batch_norm,\n",
    "#         activation_fn=tf.nn.relu,\n",
    "    )\n",
    "    y = tf.contrib.layers.fully_connected(\n",
    "        inputs=tf.contrib.layers.batch_norm(tf.nn.relu(h)),\n",
    "        num_outputs=NUM_UNITS,\n",
    "#         normalizer_fn=tf.contrib.layers.batch_norm,\n",
    "#         activation_fn=tf.nn.relu,\n",
    "    )\n",
    "    return x + y\n",
    "\n",
    "def resnet(x, depth):\n",
    "    res = tf.contrib.layers.fully_connected(\n",
    "        #inputs=x, \n",
    "        inputs = tf.nn.dropout(x, 0.5),\n",
    "        num_outputs=NUM_UNITS,\n",
    "        normalizer_fn=tf.contrib.layers.batch_norm,\n",
    "        activation_fn=tf.nn.relu,\n",
    "    )\n",
    "\n",
    "    for i in range(depth):\n",
    "        res = resblock(res)\n",
    "    return res\n",
    "\n",
    "class Model:\n",
    "    def __init__(self, sess, idim, odim, keep_rate, checkpoint_load, checkpoint_save):\n",
    "        \n",
    "        self.op_shopping = tf.placeholder(np.float32, [None, 1000])\n",
    "        self.op_app = tf.placeholder(np.float32, [None, 1000])\n",
    "        self.op_feed = tf.placeholder(np.float32, [None, 1000])\n",
    "        self.op_query = tf.placeholder(np.float32, [None, 1000])\n",
    "        self.op_xgb = tf.placeholder(np.float32, [None, 48])\n",
    "        \n",
    "        self.op_Y = tf.placeholder(np.int32, [None])\n",
    "        self.keep_rate = tf.constant(keep_rate)\n",
    "        self.op_global_step = tf.Variable(1, trainable=False, name='global_step')\n",
    "        self.op_learning_rate = tf.train.exponential_decay(1e-4, self.op_global_step, 5000, 0.9) + 1e-5\n",
    "        \n",
    "        res = tf.concat([self.op_shopping, self.op_app, self.op_feed, self.op_query, self.op_xgb], 1)\n",
    "        res = resnet(res, 4)\n",
    "\n",
    "        h2 = tf.contrib.layers.fully_connected(\n",
    "            inputs=res, \n",
    "            num_outputs=odim,\n",
    "        )\n",
    "        self.logits = h2\n",
    "\n",
    "        self.op_loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels = self.op_Y, logits = self.logits))\n",
    "        \n",
    "        optimizer = tf.train.MomentumOptimizer(self.op_learning_rate, 0.9)\n",
    "        grads_and_vars = optimizer.compute_gradients(self.op_loss)\n",
    "        grads = [grad for grad, var in grads_and_vars]\n",
    "        clipped_grads, global_norm = tf.clip_by_global_norm(grads, 10)\n",
    "\n",
    "        self.op_train = optimizer.apply_gradients(zip(clipped_grads, [var for grad, var in grads_and_vars]))\n",
    "\n",
    "        self.op_output = tf.nn.softmax(self.logits)\n",
    "        self.odim = odim\n",
    "        self.checkpoint_load = checkpoint_load\n",
    "        self.checkpoint_save = checkpoint_save\n",
    "        \n",
    "        self.saver = tf.train.Saver()\n",
    "        if checkpoint_load == None:\n",
    "            sess.run(tf.initialize_all_variables())\n",
    "        else:\n",
    "            self.saver.restore(sess, self.checkpoint_load)\n",
    "        self.history = []\n",
    "    \n",
    "    def train(self, sess, X, Y, loss=False, learning_rate=None):\n",
    "        ops = [self.op_train, self.op_loss]\n",
    "        shopping, app, feed, query, xgb = X\n",
    "        feed_dict = {\n",
    "            self.op_shopping: shopping,\n",
    "            self.op_app: app,\n",
    "            self.op_feed: feed,\n",
    "            self.op_query: query,\n",
    "            self.op_xgb: xgb,\n",
    "            self.op_Y: Y,\n",
    "        }\n",
    "        if learning_rate != None:\n",
    "            feed_dict[self.op_learning_rate] = learning_rate\n",
    "        \n",
    "        ans = sess.run(\n",
    "            ops,\n",
    "            feed_dict=feed_dict,\n",
    "        )\n",
    "        self.history += [ans[1]]\n",
    "        if len(self.history) % 1000 == 0:\n",
    "            self.saver.save(sess, self.checkpoint_save, self.op_global_step)\n",
    "        return ans[1:]\n",
    "    \n",
    "    def predict(self, sess, X):\n",
    "        return sess.run(\n",
    "            self.op_output,\n",
    "            feed_dict={\n",
    "                self.op_X: X,\n",
    "                self.keep_rate: 1,\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    def plot(self):\n",
    "        figure()\n",
    "        b = len(self.history) // 300 + 1\n",
    "        n = len(self.history) // b * b\n",
    "        x = np.mean(np.array(self.history[:n]).reshape((-1, b)), axis=1)\n",
    "        plot(np.arange(len(x)) * b, np.minimum(x, log(self.odim)))\n",
    "        show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/lib/python2.7/site-packages/tensorflow/python/util/tf_should_use.py:175: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From /usr/lib/python2.7/site-packages/tensorflow/python/util/tf_should_use.py:175: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "init = tf.initialize_all_variables()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "model_cvr = Model(sess, 4048, 2, 1., None, \"model-gender-2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_features(row):\n",
    "    Y = train_row[row]\n",
    "    shop_x = shop_matrix[row]\n",
    "    app_x = app_matrix[row]\n",
    "    feed_x = feed_matrix[row]\n",
    "    query_x = query_matrix[row]\n",
    "    xgb_x = xgb_matrix[row]\n",
    "    X = [shop_x, app_x, feed_x, query_x, xgb_x]\n",
    "    return X, Y\n",
    "\n",
    "#获取训练集每一行\n",
    "train_row = []\n",
    "\n",
    "for i in test_label:\n",
    "    train_row.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1806748\n"
     ]
    }
   ],
   "source": [
    "print 911479 + 895269\n",
    "#893540"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#0: loss = 0.981204\n",
      "#100: loss = 0.567364\n",
      "#200: loss = 0.614913\n",
      "#300: loss = 0.419695\n",
      "#400: loss = 0.513266\n",
      "#500: loss = 0.534415\n",
      "#600: loss = 0.645295\n",
      "#700: loss = 0.508196\n",
      "#800: loss = 0.583906\n",
      "#900: loss = 0.499742\n",
      "#1000: loss = 0.611360\n",
      "#1100: loss = 0.564563\n",
      "#1200: loss = 0.430775\n",
      "#1300: loss = 0.506156\n",
      "#1400: loss = 0.459691\n",
      "#1500: loss = 0.565612\n",
      "#1600: loss = 0.502488\n",
      "#1700: loss = 0.396105\n",
      "#1800: loss = 0.429918\n",
      "#1900: loss = 0.582383\n",
      "#2000: loss = 0.596589\n",
      "#2100: loss = 0.507341\n",
      "#2200: loss = 0.415008\n",
      "#2300: loss = 0.508158\n",
      "#2400: loss = 0.460174\n",
      "#2500: loss = 0.476030\n",
      "#2600: loss = 0.617267\n",
      "#2700: loss = 0.448216\n",
      "#2800: loss = 0.414999\n",
      "#2900: loss = 0.470626\n",
      "#3000: loss = 0.441852\n",
      "#3100: loss = 0.453750\n",
      "#3200: loss = 0.539206\n",
      "#3300: loss = 0.437571\n",
      "#3400: loss = 0.522737\n",
      "#3500: loss = 0.499800\n",
      "#3600: loss = 0.486756\n",
      "#3700: loss = 0.435853\n",
      "#3800: loss = 0.368289\n",
      "#3900: loss = 0.427188\n",
      "#4000: loss = 0.372047\n",
      "#4100: loss = 0.421583\n",
      "#4200: loss = 0.455774\n",
      "#4300: loss = 0.486210\n",
      "#4400: loss = 0.346419\n",
      "#4500: loss = 0.363858\n",
      "#4600: loss = 0.523381\n",
      "#4700: loss = 0.385520\n",
      "#4800: loss = 0.527445\n",
      "#4900: loss = 0.552470\n",
      "#5000: loss = 0.496554\n",
      "#5100: loss = 0.491699\n",
      "#5200: loss = 0.433371\n",
      "#5300: loss = 0.557296\n",
      "#5400: loss = 0.405350\n",
      "#5500: loss = 0.541016\n",
      "#5600: loss = 0.436738\n",
      "#5700: loss = 0.445144\n",
      "#5800: loss = 0.458423\n",
      "#5900: loss = 0.527316\n",
      "#6000: loss = 0.374710\n",
      "#6100: loss = 0.509923\n",
      "#6200: loss = 0.628656\n",
      "#6300: loss = 0.556439\n",
      "#6400: loss = 0.424264\n",
      "#6500: loss = 0.374561\n",
      "#6600: loss = 0.386576\n",
      "#6700: loss = 0.460019\n",
      "#6800: loss = 0.353008\n",
      "#6900: loss = 0.441755\n",
      "#7000: loss = 0.508456\n",
      "#7100: loss = 0.417105\n",
      "#7200: loss = 0.372206\n",
      "#7300: loss = 0.438553\n",
      "#7400: loss = 0.376300\n",
      "#7500: loss = 0.414121\n",
      "#7600: loss = 0.487221\n",
      "#7700: loss = 0.368562\n",
      "#7800: loss = 0.459221\n",
      "#7900: loss = 0.487664\n",
      "#8000: loss = 0.623450\n",
      "#8100: loss = 0.419482\n",
      "#8200: loss = 0.540766\n",
      "#8300: loss = 0.489227\n",
      "#8400: loss = 0.380105\n",
      "#8500: loss = 0.391847\n",
      "#8600: loss = 0.470835\n",
      "#8700: loss = 0.455880\n",
      "#8800: loss = 0.461707\n",
      "#8900: loss = 0.518808\n",
      "#9000: loss = 0.457867\n",
      "#9100: loss = 0.463317\n",
      "#9200: loss = 0.397240\n",
      "#9300: loss = 0.469350\n",
      "#9400: loss = 0.438326\n",
      "#9500: loss = 0.392635\n",
      "#9600: loss = 0.483831\n",
      "#9700: loss = 0.433274\n",
      "#9800: loss = 0.407303\n",
      "#9900: loss = 0.406853\n",
      "#10000: loss = 0.523270\n",
      "#10100: loss = 0.400699\n",
      "#10200: loss = 0.472123\n",
      "#10300: loss = 0.460529\n",
      "#10400: loss = 0.465453\n",
      "#10500: loss = 0.466860\n",
      "#10600: loss = 0.657089\n",
      "#10700: loss = 0.408737\n",
      "#10800: loss = 0.381028\n",
      "#10900: loss = 0.366679\n",
      "#11000: loss = 0.453043\n",
      "#11100: loss = 0.409770\n",
      "#11200: loss = 0.404063\n",
      "#11300: loss = 0.453382\n",
      "#11400: loss = 0.396717\n",
      "#11500: loss = 0.485037\n",
      "#11600: loss = 0.447953\n",
      "#11700: loss = 0.488220\n",
      "#11800: loss = 0.546886\n",
      "#11900: loss = 0.391007\n",
      "#12000: loss = 0.443437\n",
      "#12100: loss = 0.375443\n",
      "#12200: loss = 0.396277\n",
      "#12300: loss = 0.422061\n",
      "#12400: loss = 0.490280\n",
      "#12500: loss = 0.327402\n",
      "#12600: loss = 0.418577\n",
      "#12700: loss = 0.439251\n",
      "#12800: loss = 0.346142\n",
      "#12900: loss = 0.437043\n",
      "#13000: loss = 0.504815\n",
      "#13100: loss = 0.431329\n",
      "#13200: loss = 0.404704\n",
      "#13300: loss = 0.411314\n",
      "#13400: loss = 0.496781\n",
      "#13500: loss = 0.499256\n",
      "#13600: loss = 0.493041\n",
      "#13700: loss = 0.417056\n",
      "#13800: loss = 0.414845\n",
      "#13900: loss = 0.599510\n",
      "#14000: loss = 0.508714\n",
      "#14100: loss = 0.343409\n",
      "#14200: loss = 0.373438\n",
      "#14300: loss = 0.478773\n",
      "#14400: loss = 0.437785\n",
      "#14500: loss = 0.365622\n",
      "#14600: loss = 0.408975\n",
      "#14700: loss = 0.388627\n",
      "#14800: loss = 0.515746\n",
      "#14900: loss = 0.363516\n",
      "#15000: loss = 0.458251\n",
      "#15100: loss = 0.486404\n",
      "#15200: loss = 0.525957\n",
      "#15300: loss = 0.458209\n",
      "#15400: loss = 0.334002\n",
      "#15500: loss = 0.349884\n",
      "#15600: loss = 0.336349\n",
      "#15700: loss = 0.447785\n",
      "#15800: loss = 0.499472\n",
      "#15900: loss = 0.457126\n",
      "#16000: loss = 0.449249\n",
      "#16100: loss = 0.472081\n",
      "#16200: loss = 0.370828\n",
      "#16300: loss = 0.334046\n",
      "#16400: loss = 0.517030\n",
      "#16500: loss = 0.491536\n",
      "#16600: loss = 0.523607\n",
      "#16700: loss = 0.423315\n",
      "#16800: loss = 0.412276\n",
      "#16900: loss = 0.494904\n",
      "#17000: loss = 0.437177\n",
      "#17100: loss = 0.410559\n",
      "#17200: loss = 0.434794\n",
      "#17300: loss = 0.356338\n",
      "#17400: loss = 0.432106\n",
      "#17500: loss = 0.434585\n",
      "#17600: loss = 0.510284\n",
      "#17700: loss = 0.459750\n",
      "#17800: loss = 0.412342\n",
      "#17900: loss = 0.473447\n",
      "#18000: loss = 0.336801\n",
      "#18100: loss = 0.422978\n",
      "#18200: loss = 0.554591\n",
      "#18300: loss = 0.404340\n",
      "#18400: loss = 0.424842\n",
      "#18500: loss = 0.448282\n",
      "#18600: loss = 0.411772\n",
      "#18700: loss = 0.438950\n",
      "#18800: loss = 0.402391\n",
      "#18900: loss = 0.380925\n",
      "#19000: loss = 0.464280\n",
      "#19100: loss = 0.446984\n",
      "#19200: loss = 0.483165\n",
      "#19300: loss = 0.431311\n",
      "#19400: loss = 0.453810\n",
      "#19500: loss = 0.503465\n",
      "#19600: loss = 0.389102\n",
      "#19700: loss = 0.443094\n",
      "#19800: loss = 0.541834\n",
      "#19900: loss = 0.389528\n",
      "#20000: loss = 0.343930\n",
      "#20100: loss = 0.503238\n",
      "#20200: loss = 0.435442\n",
      "#20300: loss = 0.395045\n",
      "#20400: loss = 0.500090\n",
      "#20500: loss = 0.490777\n",
      "#20600: loss = 0.348151\n",
      "#20700: loss = 0.414420\n",
      "#20800: loss = 0.303906\n",
      "#20900: loss = 0.436520\n",
      "#21000: loss = 0.409373\n",
      "#21100: loss = 0.443982\n",
      "#21200: loss = 0.373280\n",
      "#21300: loss = 0.389542\n",
      "#21400: loss = 0.442462\n",
      "#21500: loss = 0.425997\n",
      "#21600: loss = 0.436303\n",
      "#21700: loss = 0.498847\n",
      "#21800: loss = 0.462808\n",
      "#21900: loss = 0.304461\n",
      "#22000: loss = 0.434130\n",
      "#22100: loss = 0.397234\n",
      "#22200: loss = 0.382388\n",
      "#22300: loss = 0.406352\n",
      "#22400: loss = 0.399475\n",
      "#22500: loss = 0.614544\n",
      "#22600: loss = 0.341747\n",
      "#22700: loss = 0.580346\n",
      "#22800: loss = 0.493590\n",
      "#22900: loss = 0.400624\n",
      "#23000: loss = 0.329849\n",
      "#23100: loss = 0.406481\n",
      "#23200: loss = 0.322319\n",
      "#23300: loss = 0.386075\n",
      "#23400: loss = 0.468963\n",
      "#23500: loss = 0.345571\n",
      "#23600: loss = 0.538465\n",
      "#23700: loss = 0.370813\n",
      "#23800: loss = 0.367220\n",
      "#23900: loss = 0.430646\n",
      "#24000: loss = 0.414850\n",
      "#24100: loss = 0.433252\n",
      "#24200: loss = 0.355628\n",
      "#24300: loss = 0.416718\n",
      "#24400: loss = 0.468820\n",
      "#24500: loss = 0.458574\n",
      "#24600: loss = 0.334063\n",
      "#24700: loss = 0.369319\n",
      "#24800: loss = 0.396452\n",
      "#24900: loss = 0.370358\n",
      "#25000: loss = 0.455139\n",
      "#25100: loss = 0.338419\n",
      "#25200: loss = 0.495178\n",
      "#25300: loss = 0.431095\n",
      "#25400: loss = 0.349538\n",
      "#25500: loss = 0.442709\n",
      "#25600: loss = 0.406997\n",
      "#25700: loss = 0.404208\n",
      "#25800: loss = 0.384568\n",
      "#25900: loss = 0.337975\n",
      "#26000: loss = 0.390638\n",
      "#26100: loss = 0.433221\n",
      "#26200: loss = 0.433048\n",
      "#26300: loss = 0.513973\n",
      "#26400: loss = 0.472002\n",
      "#26500: loss = 0.401334\n",
      "#26600: loss = 0.329925\n",
      "#26700: loss = 0.522745\n",
      "#26800: loss = 0.390617\n",
      "#26900: loss = 0.328983\n",
      "#27000: loss = 0.521133\n",
      "#27100: loss = 0.313502\n",
      "#27200: loss = 0.367459\n",
      "#27300: loss = 0.361288\n",
      "#27400: loss = 0.327068\n",
      "#27500: loss = 0.400711\n",
      "#27600: loss = 0.394945\n",
      "#27700: loss = 0.416279\n",
      "#27800: loss = 0.406552\n",
      "#27900: loss = 0.493195\n",
      "#28000: loss = 0.431474\n",
      "#28100: loss = 0.329426\n",
      "#28200: loss = 0.405945\n",
      "#28300: loss = 0.422206\n",
      "#28400: loss = 0.507212\n",
      "#28500: loss = 0.496370\n",
      "#28600: loss = 0.386396\n",
      "#28700: loss = 0.415017\n",
      "#28800: loss = 0.435984\n",
      "#28900: loss = 0.334725\n",
      "#29000: loss = 0.440762\n",
      "#29100: loss = 0.355234\n",
      "#29200: loss = 0.466398\n",
      "#29300: loss = 0.391776\n",
      "#29400: loss = 0.451414\n",
      "#29500: loss = 0.504753\n",
      "#29600: loss = 0.494713\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-89660f8c3ce2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mT\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10000000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_cvr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-89660f8c3ce2>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mhas_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_loss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0;34m\"#%d: loss = %.6f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-c9dd6454571c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, sess, X, Y, loss, learning_rate)\u001b[0m\n\u001b[1;32m     96\u001b[0m         ans = sess.run(\n\u001b[1;32m     97\u001b[0m             \u001b[0mops\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m             \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         )\n\u001b[1;32m    100\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mans\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''\n",
    "train\n",
    "'''\n",
    "def train(model):\n",
    "    X = []\n",
    "    Y = np.zeros((batch_size,), dtype=np.int32)\n",
    "\n",
    "    for i in xrange(batch_size):\n",
    "        #随机行\n",
    "        rand_row = random.randint(1, 911479)\n",
    "        \n",
    "        x, y= get_features(rand_row - 1)\n",
    "        X += [x]\n",
    "        Y[i] = y \n",
    "    X = [np.vstack([x[y] for x in X]) for y in range(5)]\n",
    "\n",
    "    has_loss = T % 100 == 0\n",
    "    loss = model.train(sess, X, Y, has_loss, learning_rate=1e-3)\n",
    "    if has_loss:\n",
    "        print \"#%d: loss = %.6f\" % (T, loss[0])\n",
    "\n",
    "for T in xrange(10000000):\n",
    "\n",
    "    train(model_cvr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_features2(row):\n",
    "    shop_x = shop_matrix[row]\n",
    "    app_x = app_matrix[row]\n",
    "    feed_x = feed_matrix[row]\n",
    "    query_x = feed_matrix[row]\n",
    "    xgb_x = xgb_matrix[row]\n",
    "    X = [shop_x, app_x, feed_x, query_x, xgb_x]\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#预测用户\n",
    "def model_predict(model, sess, X):\n",
    "    shopping, app, feed, query, xgb = X\n",
    "    feed_dict = {\n",
    "        model.op_shopping: shopping,\n",
    "        model.op_app: app,\n",
    "        model.op_feed: feed,\n",
    "        model.op_query: query, \n",
    "        model.op_xgb: xgb,\n",
    "    }\n",
    "    return sess.run(\n",
    "        model.op_output,\n",
    "        feed_dict=feed_dict\n",
    "    )\n",
    "    \n",
    "\n",
    "def predict(dev, batch):\n",
    "\n",
    "    X = []\n",
    "    \n",
    "    for i in range(dev, batch):\n",
    "        \n",
    "        x= get_features2(i)\n",
    "        X += [x]\n",
    "        \n",
    "    X = [np.vstack([x[y] for x in X]) for y in range(5)]\n",
    "\n",
    "    return model_predict(model_cvr, sess, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#测试\n",
    "#ans1008 = []\n",
    "def cv(cev, dev):\n",
    "    loss_cvr = 0.\n",
    "\n",
    "    cnt = 0\n",
    "\n",
    "    B = 128\n",
    "    with open('ans1011_02_NN.csv', 'w') as fout:\n",
    "        print >> fout, 'prob'\n",
    "        pred_gender = predict(cev, dev)\n",
    "        for i in range(cev, dev):\n",
    "            cvr = test_label[i]\n",
    "            #ans1008.append(pred_gender[i][cvr])\n",
    "            print >> fout, '%.9f'%pred_gender[cnt][1] \n",
    "            loss_cvr -= np.log(pred_gender[cnt][cvr] + 1e-10)\n",
    "            cnt += 1\n",
    "            if cnt % 100000 == 0:\n",
    "                print loss_cvr\n",
    "\n",
    "        loss_cvr /= (dev-cev)\n",
    "        print 'logloss: cvr = %.6f' % (loss_cvr)\n",
    "\n",
    "#cv(911479, 1806748)\n",
    "#cv(1806748, 2700288)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43094.5900569\n",
      "86578.6765765\n",
      "129660.615003\n",
      "172849.97522\n",
      "215990.068801\n",
      "259387.115969\n",
      "302631.694412\n",
      "345830.252179\n",
      "logloss: cvr = 0.432422\n"
     ]
    }
   ],
   "source": [
    "cv(911479, 1806748)\n",
    "#cv(1806748, 2700288)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
